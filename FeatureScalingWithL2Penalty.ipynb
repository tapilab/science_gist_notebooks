{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does feature scaling affect logistic regression coefficients with L2 regularization?\n",
    "\n",
    "Consider a text classification problem, where each document is represented by a binary feature vector.\n",
    "\n",
    "If we don't scale the feature matrix, then multiplying one column by some scalar $W$ should have the effect of increasing the impact that feature has on classification. The reason is that the L2 penalty will have less of an effect on that coefficient -- since the feature is larger, the coefficient can be smaller, so the L2 penalty is (relatively) smaller.\n",
    "\n",
    "Below, we illustrate this on a 20 newsgroups dataset. We alter the value $W$ and print both the learned coefficient, as well as the posterior probability of a document containing that single term. We observe that the posterior indeed increases for this document, indicating that the term has a greater impact on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857 instances\n"
     ]
    }
   ],
   "source": [
    "# get data.\n",
    "categories = ['alt.atheism','talk.religion.misc']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                shuffle=True, random_state=42,\n",
    "                                remove=remove)\n",
    "print('%d instances' % len(data.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(857, 6427)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# featurize.\n",
    "vec = CountVectorizer(binary=True, min_df=2)\n",
    "data.X = vec.fit_transform(data.data)\n",
    "data.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.datasets.base.Bunch"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cross validation experiment.\n",
    "def expt(data, vec, term, term_weight, C):\n",
    "    \"\"\"\n",
    "    data.............sklearn Bunch dataset.\n",
    "    vec..............CountVectorizer\n",
    "    term.............The term to inflate.\n",
    "    term_weight......How much to inflate this term.\n",
    "    C................Inverse of L2 regularization strength, for logistic regression\n",
    "    \"\"\"\n",
    "    vocab = {f:i for i, f in enumerate(vec.get_feature_names())}\n",
    "    f1s = []\n",
    "    X = data.X.copy()\n",
    "    idx = vocab[term]\n",
    "    X[:,idx] *= term_weight\n",
    "    for train, test in KFold(len(data.data), 5, random_state=42):\n",
    "        clf = LogisticRegression(C=C)\n",
    "        clf.fit(X[train], data.target[train])\n",
    "        preds = clf.predict(X[test])\n",
    "        f1s.append(f1_score(data.target[test], preds))\n",
    "    clf.fit(X, data.target)\n",
    "    xx = np.zeros(len(vocab))\n",
    "    xx[idx] = term_weight\n",
    "    proba = clf.predict_proba(csr_matrix(xx))[0][1]\n",
    "    print('%20s\\tcoef\\tposterior' % 'term')\n",
    "    print('%20s\\t%.3f\\t%.3f' % (term, clf.coef_[0][idx], proba))\n",
    "    # return np.mean(f1s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla logreg, C=1\n",
      "                term\tcoef\tposterior\n",
      "               order\t1.124\t0.793\n",
      "term weight 5, C=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/site-packages/scipy/sparse/compressed.py:739: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                term\tcoef\tposterior\n",
      "               order\t0.728\t0.980\n",
      "term weight 10, C=1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.397\t0.985\n",
      "term weight 100, C=1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.041\t0.987\n",
      "\n",
      "\n",
      "vanilla logreg, C=.1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.444\t0.626\n",
      "term weight 10, C=.1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.229\t0.915\n"
     ]
    }
   ],
   "source": [
    "print('vanilla logreg, C=1')\n",
    "expt(data, vec, 'order', 1, 1)\n",
    "print('term weight 5, C=1')\n",
    "expt(data, vec, 'order', 5, 1)\n",
    "print('term weight 10, C=1')\n",
    "expt(data, vec, 'order', 10, 1)\n",
    "print('term weight 100, C=1')\n",
    "expt(data, vec, 'order', 100, 1)\n",
    "\n",
    "\n",
    "print('\\n\\nvanilla logreg, C=.1')\n",
    "expt(data, vec, 'order', 1, .1)\n",
    "print('term weight 10, C=.1')\n",
    "expt(data, vec, 'order', 10, .1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def expt2(data, vec, term, term_weight, C):\n",
    "    \"\"\"\n",
    "    Repeat, reducing X matrix to just one column (e.g., each document has a single feature.)\n",
    "    \"\"\"\n",
    "    vocab = {f:i for i, f in enumerate(vec.get_feature_names())}\n",
    "    f1s = []\n",
    "    X = data.X.copy()\n",
    "    idx = vocab[term]\n",
    "    X[:,idx] *= term_weight\n",
    "    X = X[:,idx]\n",
    "    for train, test in KFold(len(data.data), 5, random_state=42):\n",
    "        clf = LogisticRegression(C=C)\n",
    "        clf.fit(X[train], data.target[train])\n",
    "        preds = clf.predict(X[test])\n",
    "        f1s.append(f1_score(data.target[test], preds))\n",
    "    clf.fit(X, data.target)\n",
    "    proba = clf.predict_proba([[term_weight]])[0][1]\n",
    "    print('%20s\\tcoef\\tposterior' % 'term')\n",
    "    print('%20s\\t%.3f\\t%.3f' % (term, clf.coef_[0][0], proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla logreg, C=1\n",
      "                term\tcoef\tposterior\n",
      "               order\t1.079\t0.685\n",
      "term weight 5, C=1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.236\t0.705\n",
      "term weight 10, C=1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.118\t0.706\n",
      "term weight 100, C=1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.012\t0.706\n",
      "\n",
      "\n",
      "vanilla logreg, C=.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/site-packages/scipy/sparse/compressed.py:739: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                term\tcoef\tposterior\n",
      "               order\t0.612\t0.586\n",
      "term weight 10, C=.1\n",
      "                term\tcoef\tposterior\n",
      "               order\t0.116\t0.704\n"
     ]
    }
   ],
   "source": [
    "print('vanilla logreg, C=1')\n",
    "expt2(data, vec, 'order', 1, 1)\n",
    "print('term weight 5, C=1')\n",
    "expt2(data, vec, 'order', 5, 1)\n",
    "print('term weight 10, C=1')\n",
    "expt2(data, vec, 'order', 10, 1)\n",
    "print('term weight 100, C=1')\n",
    "expt2(data, vec, 'order', 100, 1)\n",
    "\n",
    "\n",
    "print('\\n\\nvanilla logreg, C=.1')\n",
    "expt2(data, vec, 'order', 1, .1)\n",
    "print('term weight 10, C=.1')\n",
    "expt2(data, vec, 'order', 10, .1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in all cases, increasing the feature value increases the resulting posterior for a document containing only that term."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
